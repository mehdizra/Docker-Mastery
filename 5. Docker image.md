---
tags:
  - docker
  - image
up: "[[1. Docker concept]]"
date: 2024-03-05
---
![[S04 Images Slides.pdf]]
## Using Docker Hub Registry Images:
[docker hub](https://hub.docker.com/search?q=)
[docker official images](https://github.com/docker-library/official-images/tree/master/library)
[Dockerfile commands](https://docs.docker.com/reference/dockerfile/)

## S39 The Mighty Hub Using Docker Hub Registry 
### This Lecture: Review 
- Docker Hub, "the apt package system for Containers" 
- Official images and how to use them
	doesn't have the root address in their names.it means they usually work with the official team of that software who makes it.
	one of the best things about official images is the document of image
	their are some versions for official images
	every version can have some tags but only one digest SHA256 code
> [!idea]  Encrypting each image saves time and storage because it prevents repeated downloads on both Docker Hub and Docker clients.

 > [!cite] Alpine is a small distro of Linux 
 > some apps like Nginx have alpine version that is a small version ready to use in alpine 
- How to discern "good" public images ?
	To trusting  non-official images we pay attention to number of stars and number of pulls  
- Using different base images like Debian or Alpine 
- The recommended tagging scheme used by Official images 
## S40 images and their layers
### This Lecture 
- Image layers 
- Union file system 
- history and inspect commands 
- Copy on write 
### History command sample
```bash
mmzare@:)Linux ~$ docker image history nginx
IMAGE          CREATED        CREATED BY                                      SIZE
a8758716bb6a   4 months ago   CMD ["nginx" "-g" "daemon off;"]                0B
<missing>      4 months ago   STOPSIGNAL SIGQUIT                              0B
<missing>      4 months ago   EXPOSE map[80/tcp:{}]                           0B
<missing>      4 months ago   ENTRYPOINT ["/docker-entrypoint.sh"]            0B
<missing>      4 months ago   COPY 30-tune-worker-processes.sh /docker-ent…   4.62kB
<missing>      4 months ago   COPY 20-envsubst-on-templates.sh /docker-ent…   3.02kB
<missing>      4 months ago   COPY 15-local-resolvers.envsh /docker-entryp…   298B 
<missing>      4 months ago   COPY 10-listen-on-ipv6-by-default.sh /docker…   2.12kB
<missing>      4 months ago   COPY docker-entrypoint.sh / # buildkit          1.62kB
<missing>      4 months ago   RUN /bin/sh -c set -x     && groupadd --syst…   112MB
<missing>      4 months ago   ENV PKG_RELEASE=1~bookworm                      0B
<missing>      4 months ago   ENV NJS_VERSION=0.8.2                           0B
<missing>      4 months ago   ENV NGINX_VERSION=1.25.3                        0B
<missing>      4 months ago   LABEL maintainer=NGINX Docker Maintainers <d…   0B
<missing>      4 months ago   /bin/sh -c #(nop)  CMD ["bash"]                 0B
<missing>      4 months ago   /bin/sh -c #(nop) ADD file:9deb26e1dbc258df4…   74.8MB
```

### Image layers:
This is where the fundamental concept of the cache of image layers saves us a whole bunch of time and space. Because we don't need to download layers we already have, and remember it uses a unique SHA for each layer so it's guaranteed to be the exact layer it needs. It knows how to match them between Docker Hub and our local cache. As we make changes to our images, they create more layers.

If we decide that we want to have the same image be the base image for more layers, then it's only ever storing one copy of each layer.
In this system, really, one of the biggest benefits is that we're never storing the same image data more than once on our file system. It also means that when we're uploading and downloading we don't need to upload and download the same layers that we already have

### Copy On Write (COW)
How does Image layering work with containers?
If you have your image, let's say we have our Apache image, and we decide to run a container off of it, all Docker does is it creates a new read/write layer for that container on top of that Apache image. 
When we're perusing the file system and these things, all the containers and the images all just look like a regular file system, but underneath the storage driver that's used by Docker is actually layering, like a stack of pancakes, all these changes on top of each other.
So if I ran two containers at the same time off of the same Apache image, container 1 and container 2 would only be showing, in terms of the file space, they would only be the differencing between what's happened on that live container running and what is happening in the base image, which is read-only.
When you're running Containers and you're changing files that were coming through the image, let's say I started container 3, and I actually went in and changed a file that was in this image in the running container... This is known as copy-on-write.
What that does is the file system will take that file out of the image and copy it into this differencing up here... and store a copy of that file in the container layer.
So now the container is really only just the running process and those files that are different than they were in the Apache image.

 > [!cite] Images made up of two parts:
 > 1. binaries and its dependencies 
 > 2. the metadata about that image 

### inspect
show the metadata of image. the basic data that show us how the image should be run
```bash
docker image inspect
```

### Image and Their Layers: Review 
- Images are made up of file system changes and metadata 
- Each layer is uniquely identified and only stored once on a host 
- This saves storage space on host and transfer time on push/pull 
- A container is just a single read/write layer on top of image 
- docker image history and inspect commands can teach us 
مواردی که در این جلسه آموختیم:
- سیستم هوشمند لایه بندی ایمیج ها در داکر به این صورت است که هر لایه یک کد رمزنگاری شده یکتا دارد و بر این مبنا در صورتی که این کد رمزنگاری شده روی سرور (هاب یا هاست) وجود داشته باشند آن لایه مجددا دانلود یا آپلود نخواهد شد و این امر از انتقال داده های تکراری جلوگیری میکند.
- **هر کانتینری که اجرا می شود یک لایه بالایی روی لایه های ایمیج است و اگر در حین اجرا تصمیم به تغییر یکی از فایل های ایمیج داشته باشیم، آن فایل به لایه بالا (کانتینر در حال اجرا) کپی می شود و در همان لایه تغییرات اعمال می شود که به این فرایند COW گفته می شود.**
## S41 Image tagging and Pushing to Docker Hub
### This Lecture 
- All about image tags 
- How to upload to Docker Hub 
- Image ID vs. Tag 
### Official Repositories
They live at the root namespace of the registry, so they don't need account name in front of repo name.
### Tag
It's not quite a version and it's not quite a branch. It's just a pointer to a specific image commit and could be anything
> [!idea] every image can have a lot of tag but only one image ID 
#### Tag command
```bash
mmzare@:)Linux ~$ docker image tag --help
Usage:  docker image tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
mmzare@:)Linux ~$ docker image tag alpine mehdi/alpine # without tag latest has chosen 
mmzare@:)Linux ~$ docker image tag alpine mehdi/alpine:final
mmzare@:)Linux ~$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED        SIZE
mehdi/alpine               final     05455a08881e   5 weeks ago    7.38MB
mehdi/alpine               latest    05455a08881e   5 weeks ago    7.38MB
alpine                     3.19      05455a08881e   5 weeks ago    7.38MB
alpine                     latest    05455a08881e   5 weeks ago    7.38MB
```
### Docker pull 
if we pull two same image with different tag, they're not really stored twice in the cache. because they have same image ID system doesn't download it again. but with `image ls` command we see every tag that we use during the download. (latest is the default tag)
```bash
mmzare@:)Linux ~$ docker image pull alpine:3.19
mmzare@:)Linux ~$ docker image pull alpine
mmzare@:)Linux ~$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED        SIZE
alpine                     3.19      05455a08881e   5 weeks ago    7.38MB
alpine                     latest    05455a08881e   5 weeks ago    7.38MB
```
#### Docker login
to pushing data over docker hub we should sign up and login at first
```bash
mmzare@:)Linux ~$ docker login
# Log in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one. You can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/
Username: mmzare@gmail.com
Password:
Login Succeeded
```
authentication key will stored in `~/.docker/config.json` and we should log out, when we are done
```bash
mmzare@:)Linux ~$ cat .docker/config.json
{
        "auths": {
                "https://index.docker.io/v1/": {}
        },
        "credsStore": "desktop.exe"
}
mmzare@:)Linux ~$ docker logout
Removing login credentials for https://index.docker.io/v1/
mmzare@:)Linux ~$ cat .docker/config.json
{
        "auths": {},
        "credsStore": "desktop.exe"
}
```
### Docker Push
```bash
mmzare@:)Linux ~$ docker push --help
Usage:  docker push [OPTIONS] NAME[:TAG]
Upload an image to a registry
```
### Private Docker Hub Images
to create private docker hub images we should create it from Dockerhub website
![[Pasted image 20240306122020.png]]
## This Lecture: Review 
- Properly tagging images 
- Tagging images for upload to Docker Hub 
- How tagging is related to image ID 
- The Latest Tag 
- Logging into Docker Hub from docker cli 
- How to create private Docker Hub images

## S42 Lets Dive into the Docker File
Docker file is a recipe for creating image 
Docker file is not a shell script or batch file. it's a file unique to Docker and the default name is `Dockerfile` with capital D
در زمانی که بخواهیم داکرفایل دیفالت را تغییر دهیم میتوانیم از این دستور استفاده کنیم که ابزاری رایج بین ابزارهای داکر است.
```bash
mmzare@:)Linux ~$ docker build -f new_dockerfile
```
### From
- این دستور در تمامی dockerfile ها وجود دارد، چون به یک سیستم عامل در پایینترین لایه ایمیج نیاز داریم که نرم افزارها و اطلاعات مورد نیاز کانتینر مربوطه را بر روی آن سوار کنیم.
- ما از توزیع های سبک لینوکس مثل [[3. Docker container#Alpine]] استفاده میکنیم که حجم کمتری را اشغال میکنند. 
- چون این لایه ها از ایمیج رسمی دانلود می شود، میتوانیم همیشه به آبدیت بودن و امنیت بالای آن اطمینان داشته باشیم.
- از منافع دیگر استفاده از توزیع های لینوکس استفاده از Package management آنها برای نصب ابزارهای مورد نیاز دیگر است.
### ENV
- این بخش برای تعریف متغیر های محیطی است که در کانتینر بسیار مهم است چون راه اصلی برای ایجاد key/value ها در ساخت و اجرای کانتینر است.
- این متغییر ها جایی تعریف می شوند که تمامی لایه های بعدی بتوانند از آن استفاده کنند.
> [!idea]  Each one of these stanzas is a layer and the order of them is important because it does work top to down.

### Run
- در این بخش تمامی کارهای اجرایی مثل نصب یک برنامه یا به روزرسانی یا اجرای یک shell script یا تغییر یک فایل که باید داخل کانتینر انجام شود را به شکل دستوراتی بلند ایجاد می کنیم
- با استفاده از `&&` میتوانیم چندین دستور را در یک بخش قرار دهیم که در برخی مواقع این موضوع بسیار مهم است.
- دستور Run به تمامی دستوراتی که در توزیع نصب شده در بخش From وجود دارد، دسترسی دارد
- میتوانیم چندین Run در یک فایل داشته باشیم.
- نکته مهم: نباید داخل کانتینر فایل لاگ تعریف کنیم و فقط کافیست آنها را به خروجی Stdout و Stderr ارسال کنیم. درایور لاگ داکر لاگ های دریافتی از همه کانتینر ها را ذخیره و مدیریت خواهد کرد. و این از پیچیدگی کانتینر ها جلوگیری میکند.
 > [!cite] Logging
 > Docker actually handles all of our logging for us. All we have to do inside the container is make sure that everything we want to be captured in the logs is spit to Stdout and Stderr and docker will handle the rest
> There is a logging drivers that we can use in the Docker Engine itself to control all the logs

### Expose
هیچ کانتینری به هیچ شبکه ای وصل نمی شود مگر اینکه ما در این بخش برای آن پورت تعریف کنیم. و این به معنای اتصال اتوماتیک نخواهد بود اما قابلیت آن فراهم می شود.
### CMD
در این بخش یک دستور پیش فرض برای زمانی که میخواهیم یک کانتینر را Run یا start کنیم تعریف میکنیم که این دستور در هنگام run میتواند قابل تغییر باشد.
### Dockerfile Sample
```bash
# NOTE: this example is taken from the default Dockerfile for the official nginx Docker Hub Repo
# https://hub.docker.com/_/nginx/
# NOTE: This file is slightly different than the video, because nginx versions have been updated 
#       to match the latest standards from docker hub... but it's doing the same thing as the video
#       describes

FROM debian:bullseye-slim
# all images must have a FROM
# usually from a minimal Linux distribution like debian or (even better) alpine
# if you truly want to start with an empty container, use FROM scratch

LABEL maintainer="NGINX Docker Maintainers <docker-maint@nginx.com>"

# optional environment variable that's used in later lines and set as envvar when container is running
ENV NGINX_VERSION   1.23.4
ENV NJS_VERSION     0.7.11
ENV PKG_RELEASE     1~bullseye


RUN set -x \
# create nginx user/group first, to be consistent throughout docker variants
    && addgroup --system --gid 101 nginx \
    && adduser --system --disabled-login --ingroup nginx --no-create-home --home /nonexistent --gecos "nginx user" --shell /bin/false --uid 101 nginx \
    && apt-get update \
    && apt-get install --no-install-recommends --no-install-suggests -y gnupg1 ca-certificates \
    && \
    NGINX_GPGKEY=573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62; \
    NGINX_GPGKEY_PATH=/usr/share/keyrings/nginx-archive-keyring.gpg; \
    export GNUPGHOME="$(mktemp -d)"; \
    found=''; \
    for server in \
        hkp://keyserver.ubuntu.com:80 \
        pgp.mit.edu \
    ; do \
        echo "Fetching GPG key $NGINX_GPGKEY from $server"; \
        gpg1 --keyserver "$server" --keyserver-options timeout=10 --recv-keys "$NGINX_GPGKEY" && found=yes && break; \
    done; \
    test -z "$found" && echo >&2 "error: failed to fetch GPG key $NGINX_GPGKEY" && exit 1; \
    gpg1 --export "$NGINX_GPGKEY" > "$NGINX_GPGKEY_PATH" ; \
    rm -rf "$GNUPGHOME"; \
    apt-get remove --purge --auto-remove -y gnupg1 && rm -rf /var/lib/apt/lists/* \
    && dpkgArch="$(dpkg --print-architecture)" \
    && nginxPackages=" \
        nginx=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-xslt=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-geoip=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-image-filter=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-njs=${NGINX_VERSION}+${NJS_VERSION}-${PKG_RELEASE} \
    " \
    && case "$dpkgArch" in \
        amd64|arm64) \
# arches officialy built by upstream
            echo "deb [signed-by=$NGINX_GPGKEY_PATH] https://nginx.org/packages/mainline/debian/ bullseye nginx" >> /etc/apt/sources.list.d/nginx.list \
            && apt-get update \
            ;; \
        *) \
# we're on an architecture upstream doesn't officially build for
# let's build binaries from the published source packages
            echo "deb-src [signed-by=$NGINX_GPGKEY_PATH] https://nginx.org/packages/mainline/debian/ bullseye nginx" >> /etc/apt/sources.list.d/nginx.list \
            \
# new directory for storing sources and .deb files
            && tempDir="$(mktemp -d)" \
            && chmod 777 "$tempDir" \
# (777 to ensure APT's "_apt" user can access it too)
            \
# save list of currently-installed packages so build dependencies can be cleanly removed later
            && savedAptMark="$(apt-mark showmanual)" \
            \
# build .deb files from upstream's source packages (which are verified by apt-get)
            && apt-get update \
            && apt-get build-dep -y $nginxPackages \
            && ( \
                cd "$tempDir" \
                && DEB_BUILD_OPTIONS="nocheck parallel=$(nproc)" \
                    apt-get source --compile $nginxPackages \
            ) \
# we don't remove APT lists here because they get re-downloaded and removed later
            \
# reset apt-mark's "manual" list so that "purge --auto-remove" will remove all build dependencies
# (which is done after we install the built packages so we don't have to redownload any overlapping dependencies)
            && apt-mark showmanual | xargs apt-mark auto > /dev/null \
            && { [ -z "$savedAptMark" ] || apt-mark manual $savedAptMark; } \
            \
# create a temporary local APT repo to install from (so that dependency resolution can be handled by APT, as it should be)
            && ls -lAFh "$tempDir" \
            && ( cd "$tempDir" && dpkg-scanpackages . > Packages ) \
            && grep '^Package: ' "$tempDir/Packages" \
            && echo "deb [ trusted=yes ] file://$tempDir ./" > /etc/apt/sources.list.d/temp.list \
# work around the following APT issue by using "Acquire::GzipIndexes=false" (overriding "/etc/apt/apt.conf.d/docker-gzip-indexes")
#   Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied)
#   ...
#   E: Failed to fetch store:/var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages  Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied)
            && apt-get -o Acquire::GzipIndexes=false update \
            ;; \
    esac \
    \
    && apt-get install --no-install-recommends --no-install-suggests -y \
                        $nginxPackages \
                        gettext-base \
                        curl \
    && apt-get remove --purge --auto-remove -y && rm -rf /var/lib/apt/lists/* /etc/apt/sources.list.d/nginx.list \
    \
# if we have leftovers from building, let's purge them (including extra, unnecessary build deps)
    && if [ -n "$tempDir" ]; then \
        apt-get purge -y --auto-remove \
        && rm -rf "$tempDir" /etc/apt/sources.list.d/temp.list; \
    fi \
# forward request and error logs to docker log collector
    && ln -sf /dev/stdout /var/log/nginx/access.log \
    && ln -sf /dev/stderr /var/log/nginx/error.log \
# create a docker-entrypoint.d directory
    && mkdir /docker-entrypoint.d

# COPY docker-entrypoint.sh /
# COPY 10-listen-on-ipv6-by-default.sh /docker-entrypoint.d
# COPY 20-envsubst-on-templates.sh /docker-entrypoint.d
# COPY 30-tune-worker-processes.sh /docker-entrypoint.d
# ENTRYPOINT ["/docker-entrypoint.sh"]

EXPOSE 80
# expose these ports on the docker virtual network
# you still need to use -p or -P to open/forward these ports on host

STOPSIGNAL SIGQUIT

CMD ["nginx", "-g", "daemon off;"]
# required: run this command when container is launched
# only one CMD allowed, so if there are multiple, last one wins

```
### Prune
You can use "prune" commands to clean up images, volumes, build cache, and containers. Examples include:

- `docker image prune` to clean up just "dangling" images
- `docker system prune` will clean up everything
- The big one is usually `docker image prune -a` which will remove all images you're not using. Use `docker system df` to see space usage.

Remember each one of those commands has options you can learn with `--help`.

Here's a YouTube video I made about it: [https://youtu.be/_4QzP7uwtvI](https://youtu.be/_4QzP7uwtvI)

Lastly, realize that if you're using Docker Toolbox, the Linux VM won't auto-shrink. You'll need to delete it and re-create (make sure anything in docker containers or volumes are backed up). You can recreate the toolbox default VM with `docker-machine rm default` and then `docker-machine create`