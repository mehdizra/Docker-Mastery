---
tags:
  - network
  - docker
  - swarm
up: "[[3. Swarm Cluster]]"
dow: 
date: 2024-05-12
---
![[8. Swarm Basic Features and How to Use Them In Your Workflow __ 1.2 S08 Swarm Basic Features Slides.pdf]]

## Overlay Network
- با دستور `network create --driver overlay` ایجاد می شود.
- درایور overlay حجم صرفا ترافیک داخل سوارم را مدیریت میکند، باعث می شود ما دائماً با تنظیمات شبکه در نٌدهای جداگانه درگیر نباشیم.
- این ترافیک میتواند از طریق تونل IPSec رمزنگاری شود که این آپشن به صورت پیش فرض به خاطر کاهش سرعت خاموش است.
- هر سرویس می تواند به (هیچ یا یک یا چند) شبکه overlay متصل باشد.

### تفاوت شبکه ها در زمان فعال و غیر فعال بودن swarm
```bash
# swarm inactive
[node3] (local) root@192.168.0.26 ~
$ docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
125e1f541ccb   bridge    bridge    local
38c21f7f0841   host      host      local
51849e593da0   none      null      local
```
```bash
# swarm activ
[node1] (local) root@192.168.0.28 ~
$ docker network ls
NETWORK ID     NAME              DRIVER    SCOPE
9d064a125143   bridge            bridge    local
ec708d52abe6   docker_gwbridge   bridge    local
e84711d40c34   host              host      local
0zj04n9k3u6v   ingress           overlay   swarm
80277512a504   none              null      local
```
دو شبکه جدید با فعال شدن swarm اضافه می شوند که `docker_gwbridge` مربوط به ارتباطات swarm با خارج است. و `ingress` مربوط به ارتباطات داخل swarm است.

اگر ما این دو سرویس را که شامل یک `postgress database` و یک `drupal` است را روی یک سوارم با 3 نٌد فعال کنیم:
```bash
[node1] (local) root@192.168.0.28 ~
$ docker service create --network mynet --name dbase -e POSTGRESS_PASSWORD=mypass postgres
```
```bash
[node2] (local) root@192.168.0.27 ~
$ docker service create --name website --network mynet -p 80:80 drupal
```
با بررسی IP های هر سه نٌد در یک browser مشاهده خواهیم کرد که هر سه نٌد (با ipهای مجزا) که عضو سوارم هستند وب سایت را نمایش می دهند. چرا؟ Routing Mesh
## Routing Mesh
- Routes ingress (incoming) packets for a Service to proper Task 
- Spans all nodes in Swarm - Uses IPVS from Linux Kernel 
- Load balances Swarm Services across their Tasks 
- Two ways this works: 
- Container-to-container in a Overlay network (uses VIP) 
- External traffic incoming to published ports (all nodes listen)
- روتینگ مش یک شبکه ingress (incoming) است که پکت های یک سرویس را روی Task ها(containerها)ی آن سرویس بر روی هر نٌد توزیع میکند. چرا که روی هر نٌد میتوانیم سرویس ها و کانتینرهای مختلفی داشته باشیم.
- برای اینکار از قابلیت های اولیه کرنل استفاده می کند.
- هدف روتینگ مش load balancing روی تمام نٌدها و listen کردن برای ترافیک خارجی از طریق تمام نٌدهاست.
- به دو روش کار میکند:
	- در زمانی که یک سرویس ما حداقل دو replicas دارد، یک VIP (IP مجازی) و مخفی توسط داکر در جلوی آنها قرار میگیرد که وظیفه آن load balancing درون شبکه swarm است. یعنی اگر تعداد نٌدهای ما برای یک سرویس زیاد باشد، ما نیازمند بالانس کردن ترافیک داخلی آنها نیستیم. بنابراین با ایجاد یک سرویس در یک سوارم یک VIP به DNS name سرویس map می شود و DNS name یک سرویس به صورت پیش فرض نام سرویس است. مثلا فرض بگیریم سرویسی به نام my-web داریم. تمامی کانتینرهای دیگر در سرویس های دیگر در همین Swarm در صورتی که بخواهند با این سوریس صحبت کنند فقط از همان DNS name سرویس یعنی my-web استفاده میکنند که یک IP مجازی دارد و روتینگ مش پکت های ارسالی را در نٌدهای این سرویس توزیع میکند![[Pasted image 20240513150902.png]]
	- در زمانی که ما در یک Swarm سرویس های مختلفی راه اندازی میکنیم، تمامی نٌدهای این سرویس ها به پورتی که publish شده گوش می دهند و ترافیکی خارجی را به swarm load balancer یا همان routing mesh تحویل میدهند و روتینگ مش آن را به کانتینر مربوطه تحویل می دهد. این یعنی ما نیازمند انجام تنظیمات DNS و Firewall برای نٌدها نیستیم یا در صورت خارج شدن یک نٌد از swarm یک نٌد با تنظیمات مورد نیاز ایجاد می شود.![[ingress-routing-mesh.jpeg]]

> [!hint] Routing Mesh VS Round Robin
> روتینگ مش با راند رابین متفاوت است اما امکان فعال سازی راند رابین هم وجود دارد. اما به جای جنگیدن با DNS برای فعال سازی راند رابین میتوانیم از همان VIP استفاده کنیم که مشابه یک لودبالانسر سخت افزاری عمل می کند.

### Routing Mesh Sample
```bash
[node1] (local) root@192.168.0.8 ~
$ docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
kf81wvsrhwglcebble73enutp *   node1      Ready     Active         Leader           24.0.7
b88dqtamodkucy4yq2ae52422     node2      Ready     Active         Reachable        24.0.7
sg84jsgsyxmfyfwsc9suhdhfu     node3      Ready     Active         Reachable        24.0.7

[node1] (local) root@192.168.0.8 ~
$ docker service create --replicas 2 -p 9200:9200 elasticsearch:2
qqsgwc3b11l4w62jf5g7ut910
overall progress: 0 out of 3 tasks 
1/3: preparing [=================================>                 ] 
2/3: preparing [=================================>                 ] 

[node1] (local) root@192.168.0.8 ~
$ curl localhost:9200
{
  "name" : "Coldblood",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "9hL8KK6mRSKXgCQL9-7Glw",
  "version" : {
    "number" : "2.4.6",
    "build_hash" : "5376dca9f70f3abef96a77f4bb22720ace8240fd",
    "build_timestamp" : "2017-07-18T12:17:44Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.4"
  },
  "tagline" : "You Know, for Search"
}
[node1] (local) root@192.168.0.8 ~
$ curl localhost:9200
{
  "name" : "Bucky III",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "AU_yzZpwRp6rXKAUMZzsMA",
  "version" : {
    "number" : "2.4.6",
    "build_hash" : "5376dca9f70f3abef96a77f4bb22720ace8240fd",
    "build_timestamp" : "2017-07-18T12:17:44Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.4"
  },
  "tagline" : "You Know, for Search"
}
[node1] (local) root@192.168.0.8 ~
```
با هر بار انجام curl روی localhost:9200 جواب های متفاوتی از نٌدهای مختلف میگیرم
### Routing Mesh Limitation: 
	- This is stateless load balancing 
	- This load balancer is at OSI in Layer 3 (TCP), not Layer 4 (DNS) 
#### Solutions: 
	- Using Nginx or HAProxy as a load balancer proxy
	- Using Docker Enterprise Edition, which comes with built-in Layer4 web proxy
- با لودبالانسر routing mesh امکان ایجاد ارتباط دائم یک کانتینر با یک کلاینت نیست و اصطلاحا session ها ذخیره نمی شوند چرا که این یک لود بالانسر در لایه سوم `IP` و در لایه چهارم `DNS` قرار ندارد
- اگر بخواهیم چندین وب سایت را روی یک پورت، همچنین یک سرور راه اندازی کنیم نمیتوانیم ازین لود بالانسر استفاده کنیم
- برای هر دو مشکل بالا راه حل موجود استفاده از یک وب پراکسی به عنوان لایه لود بالانسر است که ارتباطات کلاینت و سرور در cache خود نگاه میدارد.
	- برای این امر هم میتوانیم از Nginx یا HAProxy استفاده میکنیم و هم میتوانیم از نسخه پولی Docker Enterprise استفاده کنیم که یک وب پراکسی داخلی درون خودش دارد و این مشکل را حل میکند.

## docker swarm app assignment
برای این امتحان فایل زیر را بخوانید:
### Assignment File
![[2Projects🌽/Docker Mastery/SWARM/files/README]]
### Assignment Answer
```bash
mmzare@:)Linux~$: docker network create --driver overlay frontend
887xz5q7jvvcufizbps6x1vpr
mmzare@:)Linux~$: docker network create --driver overlay backend
nz6drcn00v66g6ufeltyyw5m6

mmzare@:)Linux~$: docker service create --replicas 2 --name vote --network frontend -p 80:80 bretfisher/examplevotingapp_vote
k5t69fw8f9p76cyijlc76p620
overall progress: 2 out of 2 tasks
1/2: running   [==================================================>]
2/2: running   [==================================================>]
verify: Service converged

mmzare@:)Linux~$: docker service create --name redis --network frontend redis:3.2
x34ap031zf9xahos5mb2m85gv
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged

mmzare@:)Linux~$: docker service create --name worker --network frontend --network backend bretfisher/examplevotingapp_worker
d2t2pcxwjulgmf5t9yhy00ue7
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged

mmzare@:)Linux~$: docker service create --name db --network backend -e POSTGRES_HOST_AUTH_METHOD=trust --mount type=volume,source=db-data,target=/var/lib/postgresql/data postgres:9.6.1
a7vhejams8m753xyuo4vwzqzw
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged

mmzare@:)Linux~$: docker service create --name result -p 5001:80 --network backend bretfisher/examplevotingapp_result
xcaeo90qg2h8b9mn5zq2ewtfw
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged

mmzare@:)Linux~$: docker service ls
ID             NAME      MODE         REPLICAS   IMAGE                                       PORTS
a7vhejams8m7   db        replicated   1/1        postgres:9.6.1
x34ap031zf9x   redis     replicated   1/1        redis:3.2
xcaeo90qg2h8   result    replicated   1/1        bretfisher/examplevotingapp_result:latest   *:5001->80/tcp
k5t69fw8f9p7   vote      replicated   2/2        bretfisher/examplevotingapp_vote:latest     *:80->80/tcp
d2t2pcxwjulg   worker    replicated   1/1        bretfisher/examplevotingapp_worker:latest
```
با سه بروزر مختلف در آدرس localhost در رای گیری شرکت کردم و نتیجه را در localhost:5001 مشاهده کردم:
![[Pasted image 20240515130949.png]]
> [!idea] با انتخاب هر کدام از گزینه ها نتایج در بخش `result` به صورت همزمان تغییر میکرد. !
> علت آن استفاده از websocket API است.

