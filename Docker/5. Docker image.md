---
tags:
  - docker
  - image
up: "[[1. Docker concept]]"
date: 2024-03-05
---
![[S04 Images Slides.pdf]]
## Using Docker Hub Registry Images:
[docker hub](https://hub.docker.com/search?q=)
[docker official images](https://github.com/docker-library/official-images/tree/master/library)
[Dockerfile commands](https://docs.docker.com/reference/dockerfile/)

## S39 The Mighty Hub Using Docker Hub Registry 
### This Lecture: Review 
- Docker Hub, "the apt package system for Containers" 
- Official images and how to use them
	doesn't have the root address in their names.it means they usually work with the official team of that software who makes it.
	one of the best things about official images is the document of image
	their are some versions for official images
	every version can have some tags but only one digest SHA256 code
> [!idea]  Encrypting each image saves time and storage because it prevents repeated downloads on both Docker Hub and Docker clients.

 > [!cite] Alpine is a small distro of Linux 
 > some apps like Nginx have alpine version that is a small version ready to use in alpine 
- How to discern "good" public images ?
	To trusting  non-official images we pay attention to number of stars and number of pulls  
- Using different base images like Debian or Alpine 
- The recommended tagging scheme used by Official images 
## S40 images and their layers
### This Lecture 
- Image layers 
- Union file system 
- history and inspect commands 
- Copy on write 
### History command sample
```bash
mmzare@:)Linux ~$ docker image history nginx
IMAGE          CREATED        CREATED BY                  SIZE
a8758716bb6a   4 months ago   CMD ["nginx" "-g" "daemon off;"]                0B
<missing>      4 months ago   STOPSIGNAL SIGQUIT          0B
<missing>      4 months ago   EXPOSE map[80/tcp:{}]       0B
<missing>      4 months ago   ENTRYPOINT ["/docker-entrypoint.sh"]            0B
<missing>      4 months ago   COPY 30-tune-worker-processes.sh /docker-ent…   4.62kB
<missing>      4 months ago   COPY 20-envsubst-on-templates.sh /docker-ent…   3.02kB
<missing>      4 months ago   COPY 15-local-resolvers.envsh /docker-entryp…   298B 
<missing>      4 months ago   COPY 10-listen-on-ipv6-by-default.sh /docker…   2.12kB
<missing>      4 months ago   COPY docker-entrypoint.sh / # buildkit          1.62kB
<missing>      4 months ago   RUN /bin/sh -c set -x     && groupadd --syst…   112MB
<missing>      4 months ago   ENV PKG_RELEASE=1~bookworm  0B
<missing>      4 months ago   ENV NJS_VERSION=0.8.2       0B
<missing>      4 months ago   ENV NGINX_VERSION=1.25.3    0B
<missing>      4 months ago   LABEL maintainer=NGINX Docker Maintainers <d…   0B
<missing>      4 months ago   /bin/sh -c #(nop)  CMD ["bash"]                 0B
<missing>      4 months ago   /bin/sh -c #(nop) ADD file:9deb26e1dbc258df4…   74.8MB
```

### Image layers:
This is where the fundamental concept of the cache of image layers saves us a whole bunch of time and space. Because we don't need to download layers we already have, and remember it uses a unique SHA for each layer so it's guaranteed to be the exact layer it needs. It knows how to match them between Docker Hub and our local cache. As we make changes to our images, they create more layers.

If we decide that we want to have the same image as the base image for more layers, then only one copy of each layer is saved.
In this system, one of the biggest benefits is that we're never storing the same image data more than once on our file system. It also means that when we're uploading and downloading we don't need to upload and download the same layers that we already have

### Copy On Write (COW)
How does Image layering work with containers?
If you have your image, let's say we have our Apache image, and we decide to run a container off of it, all Docker does is it creates a new read/write layer for that container on top of that Apache image. 
When we're perusing the file system and these things, all the containers and the images all just look like a regular file system, but underneath the storage driver that's used by Docker is actually layering, like a stack of pancakes, all these changes on top of each other.
So if I ran two containers at the same time off of the same Apache image, container 1 and container 2 would only be showing, in terms of the file space, they would only be the differencing between what's happened on that live container running and what is happening in the base image, which is read-only.
When you're running Containers and you're changing files that were coming through the image, let's say I started container 3, and I actually went in and changed a file that was in this image in the running container... This is known as copy-on-write.
What that does is the file system will take that file out of the image and copy it into this differencing up here... and store a copy of that file in the container layer.
So now the container is really only just the running process and those files that are different than they were in the Apache image.

 > [!cite] Images made up of two parts:
 > 1. binaries and its dependencies 
 > 2. the metadata about that image 

### inspect
show the metadata of image. the basic data that show us how the image should be run
```bash
docker image inspect
```

### Image and Their Layers: Review 
- Images are made up of file system changes and metadata 
- Each layer is uniquely identified and only stored once on a host 
- This saves storage space on host and transfer time on push/pull 
- A container is just a single read/write layer on top of image 
- docker image history and inspect commands can teach us 
مواردی که در این جلسه آموختیم:
- سیستم هوشمند لایه بندی ایمیج ها در داکر به این صورت است که هر لایه یک کد رمزنگاری شده یکتا دارد و بر این مبنا در صورتی که این کد رمزنگاری شده روی سرور (هاب یا هاست) وجود داشته باشند آن لایه مجددا دانلود یا آپلود نخواهد شد و این امر از انتقال داده های تکراری جلوگیری میکند.
- **هر کانتینری که اجرا می شود یک لایه بالایی روی لایه های ایمیج است و اگر در حین اجرا تصمیم به تغییر یکی از فایل های ایمیج داشته باشیم، آن فایل به لایه بالا (کانتینر در حال اجرا) کپی می شود و در همان لایه تغییرات اعمال می شود که به این فرایند COW گفته می شود.**
## S41 Image tagging and Pushing to Docker Hub
### This Lecture 
- All about image tags 
- How to upload to Docker Hub 
- Image ID vs. Tag 
### Official Repositories
They live at the root namespace of the registry, so they don't need account name in front of repo name.
### Tag
It's not quite a version and it's not quite a branch. It's just a pointer to a specific image commit and could be anything
> [!idea] every image can have a lot of tag but only one image ID 
### Docker Image Commands
```bash
Usage:  docker image COMMAND
Manage images
Commands:
  build       Build an image from a Dockerfile
  history     Show the history of an image
  import      Import the contents from a tarball to create a filesystem image
  inspect     Display detailed information on one or more images
  load        Load an image from a tar archive or STDIN
  ls          List images
  prune       Remove unused images
  pull        Download an image from a registry
  push        Upload an image to a registry
  rm          Remove one or more images
  save        Save one or more images to a tar archive (streamed to STDOUT by default)
  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE
Run 'docker image COMMAND --help' for more information on a command.
```
#### Tag command
```bash
mmzare@:)Linux ~$ docker image tag --help
Usage:  docker image tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
mmzare@:)Linux ~$ docker image tag alpine mehdi/alpine # without tag latest has chosen 
mmzare@:)Linux ~$ docker image tag alpine mehdi/alpine:final
mmzare@:)Linux ~$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED        SIZE
mehdi/alpine               final     05455a08881e   5 weeks ago    7.38MB
mehdi/alpine               latest    05455a08881e   5 weeks ago    7.38MB
alpine                     3.19      05455a08881e   5 weeks ago    7.38MB
alpine                     latest    05455a08881e   5 weeks ago    7.38MB
```
### Docker pull 
if we pull two same image with different tag, they're not really stored twice in the cache. because they have same image ID system doesn't download it again. but with `image ls` command we see every tag that we use during the download. (latest is the default tag)
```bash
mmzare@:)Linux ~$ docker image pull alpine:3.19
mmzare@:)Linux ~$ docker image pull alpine
mmzare@:)Linux ~$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED        SIZE
alpine                     3.19      05455a08881e   5 weeks ago    7.38MB
alpine                     latest    05455a08881e   5 weeks ago    7.38MB
```
#### Docker login
to pushing data over docker hub we should sign up and login at first
```bash
mmzare@:)Linux ~$ docker login
# Log in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one. You can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/
Username: mmzare@gmail.com
Password:
Login Succeeded
```
authentication key will stored in `~/.docker/config.json` and we should log out, when we are done
```bash
mmzare@:)Linux ~$ cat .docker/config.json
{
        "auths": {
                "https://index.docker.io/v1/": {}
        },
        "credsStore": "desktop.exe"
}
mmzare@:)Linux ~$ docker logout
Removing login credentials for https://index.docker.io/v1/
mmzare@:)Linux ~$ cat .docker/config.json
{
        "auths": {},
        "credsStore": "desktop.exe"
}
```
### Docker Push
```bash
mmzare@:)Linux ~$ docker push --help
Usage:  docker push [OPTIONS] NAME[:TAG]
Upload an image to a registry
```
### Private Docker Hub Images
to create private docker hub images we should create it from Dockerhub website
![[Pasted image 20240306122020.png]]
## This Lecture: Review 
- Properly tagging images 
- Tagging images for upload to Docker Hub 
- How tagging is related to image ID 
- The Latest Tag 
- Logging into Docker Hub from docker cli 
- How to create private Docker Hub images

## S42 Lets Dive into the Docker File
Docker file is a recipe for creating image 
Docker file is not a shell script or batch file. it's a file unique to Docker and the default name is `Dockerfile` with capital D
در زمانی که بخواهیم داکرفایل دیفالت را تغییر دهیم میتوانیم از این دستور استفاده کنیم که ابزاری رایج بین ابزارهای داکر است.
```bash
mmzare@:)Linux ~$ docker build -f new_dockerfile
```
### From
- این دستور در تمامی dockerfile ها وجود دارد، چون به یک سیستم عامل در پایینترین لایه ایمیج نیاز داریم که نرم افزارها و اطلاعات مورد نیاز کانتینر مربوطه را بر روی آن سوار کنیم.
- ما از توزیع های سبک لینوکس مثل [[3. Docker container#Alpine]] استفاده میکنیم که حجم کمتری را اشغال میکنند. 
- چون این لایه ها از ایمیج رسمی دانلود می شود، میتوانیم همیشه به آبدیت بودن و امنیت بالای آن اطمینان داشته باشیم.
- از منافع دیگر استفاده از توزیع های لینوکس استفاده از Package management آنها برای نصب ابزارهای مورد نیاز دیگر است.
### ENV
- این بخش برای تعریف متغیر های محیطی است که در کانتینر بسیار مهم است چون راه اصلی برای ایجاد key/value ها در ساخت و اجرای کانتینر است.
- این متغییر ها جایی تعریف می شوند که تمامی لایه های بعدی بتوانند از آن استفاده کنند.
> [!idea]  Each one of these stanzas is a layer and the order of them is important because it does work top to down.

### Run
- در این بخش تمامی کارهای اجرایی مثل نصب یک برنامه یا به روزرسانی یا اجرای یک shell script یا تغییر یک فایل که باید داخل کانتینر انجام شود را به شکل دستوراتی بلند ایجاد می کنیم
- با استفاده از `&&` میتوانیم چندین دستور را در یک بخش قرار دهیم که در برخی مواقع این موضوع بسیار مهم است.
- دستور Run به تمامی دستوراتی که در توزیع نصب شده در بخش From وجود دارد، دسترسی دارد
- میتوانیم چندین Run در یک فایل داشته باشیم.
- نکته مهم: نباید داخل کانتینر فایل لاگ تعریف کنیم و فقط کافیست آنها را به خروجی Stdout و Stderr ارسال کنیم. درایور لاگ داکر لاگ های دریافتی از همه کانتینر ها را ذخیره و مدیریت خواهد کرد. و این از پیچیدگی کانتینر ها جلوگیری میکند.
 > [!cite] Logging
 > Docker actually handles all of our logging for us. All we have to do inside the container is make sure that everything we want to be captured in the logs is spit to Stdout and Stderr and docker will handle the rest
> There is a logging drivers that we can use in the Docker Engine itself to control all the logs
### Expose
هیچ کانتینری به هیچ شبکه ای وصل نمی شود مگر اینکه ما در این بخش برای آن پورت تعریف کنیم. و این به معنای اتصال اتوماتیک نخواهد بود اما قابلیت آن فراهم می شود.
### CMD
در این بخش یک دستور پیش فرض برای زمانی که میخواهیم یک کانتینر را Run یا start کنیم تعریف میکنیم که این دستور در هنگام run میتواند قابل تغییر باشد.
### Dockerfile Sample
```bash
# NOTE: this example is taken from the default Dockerfile for the official nginx Docker Hub Repo
# https://hub.docker.com/_/nginx/
# NOTE: This file is slightly different than the video, because nginx versions have been updated 
#       to match the latest standards from docker hub... but it's doing the same thing as the video
#       describes

FROM debian:bullseye-slim
# all images must have a FROM
# usually from a minimal Linux distribution like debian or (even better) alpine
# if you truly want to start with an empty container, use FROM scratch

LABEL maintainer="NGINX Docker Maintainers <docker-maint@nginx.com>"

# optional environment variable that's used in later lines and set as envvar when container is running
ENV NGINX_VERSION   1.23.4
ENV NJS_VERSION     0.7.11
ENV PKG_RELEASE     1~bullseye


RUN set -x \
# create nginx user/group first, to be consistent throughout docker variants
    && addgroup --system --gid 101 nginx \
    && adduser --system --disabled-login --ingroup nginx --no-create-home --home /nonexistent --gecos "nginx user" --shell /bin/false --uid 101 nginx \
    && apt-get update \
    && apt-get install --no-install-recommends --no-install-suggests -y gnupg1 ca-certificates \
    && \
    NGINX_GPGKEY=573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62; \
    NGINX_GPGKEY_PATH=/usr/share/keyrings/nginx-archive-keyring.gpg; \
    export GNUPGHOME="$(mktemp -d)"; \
    found=''; \
    for server in \
        hkp://keyserver.ubuntu.com:80 \
        pgp.mit.edu \
    ; do \
        echo "Fetching GPG key $NGINX_GPGKEY from $server"; \
        gpg1 --keyserver "$server" --keyserver-options timeout=10 --recv-keys "$NGINX_GPGKEY" && found=yes && break; \
    done; \
    test -z "$found" && echo >&2 "error: failed to fetch GPG key $NGINX_GPGKEY" && exit 1; \
    gpg1 --export "$NGINX_GPGKEY" > "$NGINX_GPGKEY_PATH" ; \
    rm -rf "$GNUPGHOME"; \
    apt-get remove --purge --auto-remove -y gnupg1 && rm -rf /var/lib/apt/lists/* \
    && dpkgArch="$(dpkg --print-architecture)" \
    && nginxPackages=" \
        nginx=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-xslt=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-geoip=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-image-filter=${NGINX_VERSION}-${PKG_RELEASE} \
        nginx-module-njs=${NGINX_VERSION}+${NJS_VERSION}-${PKG_RELEASE} \
    " \
    && case "$dpkgArch" in \
        amd64|arm64) \
# arches officialy built by upstream
            echo "deb [signed-by=$NGINX_GPGKEY_PATH] https://nginx.org/packages/mainline/debian/ bullseye nginx" >> /etc/apt/sources.list.d/nginx.list \
            && apt-get update \
            ;; \
        *) \
# we're on an architecture upstream doesn't officially build for
# let's build binaries from the published source packages
            echo "deb-src [signed-by=$NGINX_GPGKEY_PATH] https://nginx.org/packages/mainline/debian/ bullseye nginx" >> /etc/apt/sources.list.d/nginx.list \
            \
# new directory for storing sources and .deb files
            && tempDir="$(mktemp -d)" \
            && chmod 777 "$tempDir" \
# (777 to ensure APT's "_apt" user can access it too)
            \
# save list of currently-installed packages so build dependencies can be cleanly removed later
            && savedAptMark="$(apt-mark showmanual)" \
            \
# build .deb files from upstream's source packages (which are verified by apt-get)
            && apt-get update \
            && apt-get build-dep -y $nginxPackages \
            && ( \
                cd "$tempDir" \
                && DEB_BUILD_OPTIONS="nocheck parallel=$(nproc)" \
                    apt-get source --compile $nginxPackages \
            ) \
# we don't remove APT lists here because they get re-downloaded and removed later
            \
# reset apt-mark's "manual" list so that "purge --auto-remove" will remove all build dependencies
# (which is done after we install the built packages so we don't have to redownload any overlapping dependencies)
            && apt-mark showmanual | xargs apt-mark auto > /dev/null \
            && { [ -z "$savedAptMark" ] || apt-mark manual $savedAptMark; } \
            \
# create a temporary local APT repo to install from (so that dependency resolution can be handled by APT, as it should be)
            && ls -lAFh "$tempDir" \
            && ( cd "$tempDir" && dpkg-scanpackages . > Packages ) \
            && grep '^Package: ' "$tempDir/Packages" \
            && echo "deb [ trusted=yes ] file://$tempDir ./" > /etc/apt/sources.list.d/temp.list \
# work around the following APT issue by using "Acquire::GzipIndexes=false" (overriding "/etc/apt/apt.conf.d/docker-gzip-indexes")
#   Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied)
#   ...
#   E: Failed to fetch store:/var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages  Could not open file /var/lib/apt/lists/partial/_tmp_tmp.ODWljpQfkE_._Packages - open (13: Permission denied)
            && apt-get -o Acquire::GzipIndexes=false update \
            ;; \
    esac \
    \
    && apt-get install --no-install-recommends --no-install-suggests -y \
    $nginxPackages \
    gettext-base \
    curl \
    && apt-get remove --purge --auto-remove -y && rm -rf /var/lib/apt/lists/* /etc/apt/sources.list.d/nginx.list \
    \
# if we have leftovers from building, let's purge them (including extra, unnecessary build deps)
    && if [ -n "$tempDir" ]; then \
        apt-get purge -y --auto-remove \
        && rm -rf "$tempDir" /etc/apt/sources.list.d/temp.list; \
    fi \
# forward request and error logs to docker log collector
    && ln -sf /dev/stdout /var/log/nginx/access.log \
    && ln -sf /dev/stderr /var/log/nginx/error.log \
# create a docker-entrypoint.d directory
    && mkdir /docker-entrypoint.d

# COPY docker-entrypoint.sh /
# COPY 10-listen-on-ipv6-by-default.sh /docker-entrypoint.d
# COPY 20-envsubst-on-templates.sh /docker-entrypoint.d
# COPY 30-tune-worker-processes.sh /docker-entrypoint.d
# ENTRYPOINT ["/docker-entrypoint.sh"]

EXPOSE 80
# expose these ports on the docker virtual network
# you still need to use -p or -P to open/forward these ports on host

STOPSIGNAL SIGQUIT

CMD ["nginx", "-g", "daemon off;"]
# required: run this command when container is launched
# only one CMD allowed, so if there are multiple, last one wins

```

### Docker Image Build
برای ساخت ایمیج با استفاده از داکرفایل ازین دستور استفاده میکنیم. و در بخش PATH آدرس داکرفایل را می دهیم.
```bash
docker image build --help
Usage:  docker buildx build [OPTIONS] PATH | URL |
```
لایه ها از بالا به پایین ساخته می شوند و در cache داکر قرار می گیرند و به ازای هر لایه یک هش ایجاد می شود. و اگر بخواهیم تغییراتی روی dockerfile ایجاد کنیم و ایمیج را rebuild کنیم تک تک لایه ها از بالا بررسی می شوند و در صورت نبود تغییر در آن لایه از فایلی که در cache قرار گرفته استفاده می شود و در واقع آن لایه مجددا ساخته نمی شود. با مشخص شدن اولین تغییر در هر لایه، آن لایه و لایه های بعدی (حتی اگر تغییری در لایه های بعدی نباشد) مجددا ساخته می شوند و در cache قرار می گیرند و هش جدید آنها ایجاد می شود.
> [!danger]  بنابراین بسیار مهم است که تنظیمات و فایلهایی که کمتر تغییر میکنند را در لایه های بالایی و مواردی که قرار است مدام تغییر کنند را در لایه های پایینی قرار دهیم 
![[Pasted image 20240308211159.png]]
 
#### Image build sample
```bash
mmzare@:)Linux dockerfile-sample-2$ cat Dockerfile
# this shows how we can extend/change an existing official image from Docker Hub

FROM nginx:latest
# highly recommend you always pin versions for anything beyond dev/learn

WORKDIR /usr/share/nginx/html
# change working directory to root of nginx webhost
# using WORKDIR is preferred to using 'RUN cd /some/path'

COPY index.html index.html

# I don't have to specify EXPOSE or CMD because they're in my FROM
mmzare@:)Linux dockerfile-sample-2$ docker image build -t newnginx .
[+] Building 0.1s (8/8) FINISHED             docker:default
 => [internal] load build definition from Dockerfile   0.0s
 => => transferring dockerfile: 461B                   0.0s
 => [internal] load .dockerignore                      0.0s
 => => transferring context: 2B                        0.0s
 => [internal] load metadata for docker.io/library/nginx:latest            0.0s
 => [1/3] FROM docker.io/library/nginx:latest          0.0s
 => [internal] load build context                      0.0s
 => => transferring context: 32B                       0.0s
 => CACHED [2/3] WORKDIR /usr/share/nginx/html         0.0s
 => CACHED [3/3] COPY index.html index.html            0.0s
 => exporting to image             0.0s
 => => exporting layers            0.0s
 => => writing image sha256:3c7b2e5bc4c43be205227b88b5d734a0155338e2fe35acf6706cd72b6cc51736   0.0s
 => => naming to docker.io/library/newnginx           0.0s
```
> [!idea] `COPY` stanza format:
> COPY {SOURCE FILE from local machine working directory} to {TARGET FILE in image working directory}

در مثال بالا زمانی که در بخش from ایمیج nginx:latest را فراخوانی میکنیم در صورت وجود این تگ در local machine همان را استفاده میکند و آخرین ورژن را از docker hub بررسی نمیکند.
اگر آخرین ورژن را از داکرهاب pull کنیم و دوباره image build انجام دهیم فایل جدیدی ساخته می شود که هش آن متفاوت است. 
بعد از این کار ایمیج nginx:latest سابق روی سیستم تغییر تگ میدهد به حالت nginx:none و در مقابل آن توضیح (Dangling) میبینیم که به این معناست که این ایمیج دیگر به هیچ ایمیج ساخته شده ای تگ نیست و دیگر کاربردی ندارد و صرفا فضای سیستم را اشغال کرده است.
```bash
mmzare@:)Linux dockerfile-sample-2$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED          SIZE
newnginx                   latest    3c7b2e5bc4c4   5 minutes ago   187MB
mmzare@:)Linux dockerfile-sample-2$ docker pull nginx:latest
latest: Pulling from library/nginx
e1caac4eb9d2: Pull complete
mmzare@:)Linux dockerfile-sample-2$ docker image build -t newnginx2 .
mmzare@:)Linux dockerfile-sample-2$ docker image build -t newnginx3 .
mmzare@:)Linux dockerfile-sample-2$ docker image ls
REPOSITORY                 TAG       IMAGE ID       CREATED          SIZE
newnginx3                  latest    6a532201513e   2 minutes ago    187MB
newnginx2                  latest    6a532201513e   2 minutes ago    187MB
newnginx                   latest    3c7b2e5bc4c4   7 minutes ago   187MB

``` 
### (Dangling)
What is the difference between dangling and unused images?
**Dangling images are layers that have no relationship to any tagged images**. They no longer serve a purpose and consume disk space. An unused image is an image that has not been assigned or used in a container. Dangling images are untagged images.

 > [!cite] یکی از وظایف معمول داکرادمین ایجاد ایمیج های اختصاصی برای نرم افزار مورد نظر است.
 > حتی ممکن است در مورد پکیج ها و زبان برنامه نویسی نرم افزار مورد نظر اطلاعی نداشته باشد اما همچنان باید آن را Dockerize کند. که به معنای ایجاد یک dockerfile برای یک نرم افزار یا سرویس است و تبدیل آن به ایمیج و اجرای آن در یک کانتینر است.
 > که شامل پیدا کردن ایمیج های (ترجیحا) رسمی و در صورت لزوم غیررسمی و بررسی در مورد نحوه ساخت و عملکرد آنهاست و پس از آن ایجاد ایمیج های اختصاصی جدید با استفاده از آنهاست.
 > **داکر ادمین باید هنر ادغام ایمیج های موجود و ساختار نرم افزار مورد نظر را بلد باشد که نهایتا به ایجاد یک ایمیج اختصاصی منتهی شود**


### Assignment: Build Your Own Image 
• Dockerfiles are part process workflow and part art 
• Take existing Node.js app and Dockerize it <br>• Make Dockerfile. Build it. Test it. Push it. (rm it). Run it. <br>• Expect this to be iterative. Rarely do I get it right the first time. <br>• Details in dockerfile-assignment-1/Dockerfile <br>• Use the Alpine version of the official 'node' 6.x image <br>• Expected result is web site at http://localhost <br>• Tag and push to your Docker Hub account (free) <br>• Remove your image from local cache, run again from Hub
```bash
FROM node:6-alpine
EXPOSE 3000
RUN apk add --no-cache tini
RUN mkdir -p /usr/src/app && cd /usr/src/app
COPY . .
RUN apk add npm
RUN npm install && npm cache clean --force
CMD ["/sbin/tini", "--", "node", "./bin/www"]
```
> [!question] What is tini?
> [[About Tini]] 
> به صورت مختصر tini ابزاری است که پردازش هایی که بدون پردازش والد رها شده اند را مدیریت می کند (می بندد) و در کانتینرها بسیار کاربرد دارد چرا که بسیار اتفاق می افتد که پردازشی را بدون پردازش والد با PID1 ایجاد میکنیم

> [!idea] در حین ساخت ایمیج به مشکلات جالبی برخورد کردم و تجربیات مختلفی بدست آوردم:
> [[personal docker experience]]

### Prune
You can use "prune" commands to clean up images, volumes, build cache, and containers. Examples include:

- `docker image prune` to clean up just "dangling" images
- `docker system prune` will clean up everything
- The big one is usually `docker image prune -a` which will remove all images you're not using. Use `docker system df` to see space usage.

Remember each one of those commands has options you can learn with `--help`.

Here's a YouTube video Bret Fisher made about it: [https://youtu.be/_4QzP7uwtvI](https://youtu.be/_4QzP7uwtvI)

Lastly, realize that if you're using Docker Toolbox, the Linux VM won't auto-shrink. You'll need to delete it and re-create (make sure anything in docker containers or volumes are backed up). You can recreate the toolbox default VM with `docker-machine rm default` and then `docker-machine create`